{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29e94eb-83f5-4c63-81ca-0dba971b3674",
   "metadata": {
    "collapsed": true,
    "id": "d29e94eb-83f5-4c63-81ca-0dba971b3674",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import zipfile\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import multiprocessing\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import time, os, copy, argparse\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gF4NHPhInv1Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF4NHPhInv1Y",
    "outputId": "94a10a87-4d4d-4f2e-86ee-c270e04b5210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2c0dfb-f470-4ff2-a476-91da5efc08b6",
   "metadata": {
    "id": "ca2c0dfb-f470-4ff2-a476-91da5efc08b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path, dest_path):\n",
    "    \"\"\"\n",
    "    This function extracts a zip file to a specified destination path.\n",
    "\n",
    "    Parameters:\n",
    "    zip_path (str): The path to the zip file that needs to be extracted.\n",
    "    dest_path (str): The path where the zip file should be extracted to.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Open the zip file in read mode\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all the contents of the zip file to dest_path\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "# Usage\n",
    "# The function is called with the path of the zip file and the destination path as arguments\n",
    "extract_zip_file('/content/drive/MyDrive/data.zip', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7158adb9-9a9d-4c18-89b8-bcba65e6bb81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7158adb9-9a9d-4c18-89b8-bcba65e6bb81",
    "outputId": "474de5ce-48cb-4160-cdd8-3e46a002903d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "0    532\n",
      "1    507\n",
      "Name: count, dtype: int64\n",
      "(1039, 16)\n",
      "(1039, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "X_train = pd.read_csv('data/X_train.csv')  # Load features from CSV file\n",
    "Y_train = pd.read_csv('data/Y_train.csv')  # Load labels from CSV file\n",
    "\n",
    "# Add the output column to X_train\n",
    "X_train['output'] = Y_train['output']  # Add 'output' column to X_train dataframe\n",
    "\n",
    "# Print the count of unique values in the 'output' column. This is useful to understand the distribution of values.\n",
    "print(X_train['output'].value_counts())\n",
    "\n",
    "# Print the shape of the dataframes to understand the number of rows and columns\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of Y_train: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163cc75c-6f52-4a48-90c1-565724edbeac",
   "metadata": {
    "id": "163cc75c-6f52-4a48-90c1-565724edbeac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# The stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n",
    "# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n",
    "train, valid = train_test_split(X_train, test_size=0.10, stratify=X_train['output'], random_state=42)\n",
    "\n",
    "def process_df(df, folder):\n",
    "    \"\"\"\n",
    "    This function processes a dataframe and copies images to a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe to process.\n",
    "    folder (str): The folder to copy images to.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the output value (0 or 1)\n",
    "        output = row['output']\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        output_dir = os.path.join('IMAGES_', folder, str(output))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Copy the image to the output directory\n",
    "        shutil.copy(row['image_path'], output_dir)\n",
    "\n",
    "# Process the training and validation dataframes\n",
    "process_df(train, 'train')\n",
    "process_df(valid, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f7be53-0e36-4860-b85a-ba7a6fdb85e2",
   "metadata": {
    "id": "89f7be53-0e36-4860-b85a-ba7a6fdb85e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_directory = 'IMAGES_/train'\n",
    "valid_directory = 'IMAGES_/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feab8fb1-c627-453a-bd7f-8a80e2eb4732",
   "metadata": {
    "id": "feab8fb1-c627-453a-bd7f-8a80e2eb4732",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mode ='transfer'\n",
    "# Path to save the transfered model\n",
    "PATH = \"model_transfer.pth\"\n",
    "\n",
    "# Batch size for training\n",
    "bs = 8\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 10\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Number of workers for data loading\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Define the transformations to be applied to the images\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),  # Randomly resize and crop the image\n",
    "        transforms.RandomRotation(degrees=15),  # Randomly rotate the image\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "        transforms.CenterCrop(size=224),  # Crop the image from the center\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),  # Resize the image\n",
    "        transforms.CenterCrop(size=224),  # Crop the image from the center\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets from the specified folders and apply the transformations\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),  # Training dataset\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])  # Validation dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1908826a-b4d7-4b29-bb20-54716258ad2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1908826a-b4d7-4b29-bb20-54716258ad2d",
    "outputId": "28ae1a2f-8303-41db-ebca-4aa567a87507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of the training and validation datasets\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset['train']),  # Size of the training dataset\n",
    "    'valid': len(dataset['valid'])  # Size of the validation dataset\n",
    "}\n",
    "\n",
    "# Create DataLoader objects for the training and validation datasets\n",
    "# These will provide batches of data to the training loop\n",
    "# They also handle shuffling and parallel data loading\n",
    "dataloaders = {\n",
    "    'train': data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                             num_workers=num_cpu, pin_memory=True, drop_last=True),  # DataLoader for the training data\n",
    "    'valid': data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                             num_workers=num_cpu, pin_memory=True, drop_last=True)  # DataLoader for the validation data\n",
    "}\n",
    "\n",
    "# Get the class names or target labels from the training dataset\n",
    "class_names = dataset['train'].classes\n",
    "\n",
    "# Print the class names\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df79f116-4a2d-439f-88c0-23e252473f77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df79f116-4a2d-439f-88c0-23e252473f77",
    "outputId": "302a2791-bc2d-44d4-e6a0-4bb0d85155e1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-set size: 935 \n",
      "Validation-set size: 104\n",
      "\n",
      "Loading mobilenetv2 as feature extractor ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes of the training and validation datasets\n",
    "print(\"Training-set size:\", dataset_sizes['train'])\n",
    "print(\"Validation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set the default device to GPU if it's available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check the training mode\n",
    "if train_mode == 'finetune':\n",
    "    # Load a pre-trained ResNet18 model\n",
    "    print(\"\\nLoading ResNet18 for fine-tuning...\\n\")\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Get the number of features in the last layer (fc layer)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "    # Modify the last layer to match the number of classes in the dataset\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "elif train_mode == 'scratch':\n",
    "    # Load a custom VGG11 model for training from scratch\n",
    "    print(\"\\nLoading VGG11 for training from scratch...\\n\")\n",
    "    model_ft = MyVGG11(in_ch=3, num_classes=2)\n",
    "\n",
    "    # Set the number of epochs to a higher value for training from scratch\n",
    "    num_epochs = 100\n",
    "\n",
    "elif train_mode == 'transfer':\n",
    "    # Load a pre-trained MobileNetV2 model for transfer learning\n",
    "    print(\"\\nLoading MobileNetV2 as feature extractor...\\n\")\n",
    "    model_ft = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "    # Freeze all the layers except the last convolution block and fully connected (fc) layers\n",
    "    for params in list(model_ft.parameters())[0:-5]:\n",
    "        params.requires_grad = False\n",
    "\n",
    "    # Get the number of features in the last layer of the classifier\n",
    "    num_ftrs = model_ft.classifier[-1].in_features\n",
    "\n",
    "    # Modify the classifier to match the number of classes in the dataset\n",
    "    model_ft.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=False),\n",
    "        nn.Linear(in_features=num_ftrs, out_features=num_classes, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a911f71-bf86-4730-9600-aca603177a4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7a911f71-bf86-4730-9600-aca603177a4d",
    "outputId": "30b99315-22bb-4957-e117-940ad4108463",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:-\n",
      "\n",
      "0 features.0.0.weight False\n",
      "1 features.0.1.weight False\n",
      "2 features.0.1.bias False\n",
      "3 features.1.conv.0.0.weight False\n",
      "4 features.1.conv.0.1.weight False\n",
      "5 features.1.conv.0.1.bias False\n",
      "6 features.1.conv.1.weight False\n",
      "7 features.1.conv.2.weight False\n",
      "8 features.1.conv.2.bias False\n",
      "9 features.2.conv.0.0.weight False\n",
      "10 features.2.conv.0.1.weight False\n",
      "11 features.2.conv.0.1.bias False\n",
      "12 features.2.conv.1.0.weight False\n",
      "13 features.2.conv.1.1.weight False\n",
      "14 features.2.conv.1.1.bias False\n",
      "15 features.2.conv.2.weight False\n",
      "16 features.2.conv.3.weight False\n",
      "17 features.2.conv.3.bias False\n",
      "18 features.3.conv.0.0.weight False\n",
      "19 features.3.conv.0.1.weight False\n",
      "20 features.3.conv.0.1.bias False\n",
      "21 features.3.conv.1.0.weight False\n",
      "22 features.3.conv.1.1.weight False\n",
      "23 features.3.conv.1.1.bias False\n",
      "24 features.3.conv.2.weight False\n",
      "25 features.3.conv.3.weight False\n",
      "26 features.3.conv.3.bias False\n",
      "27 features.4.conv.0.0.weight False\n",
      "28 features.4.conv.0.1.weight False\n",
      "29 features.4.conv.0.1.bias False\n",
      "30 features.4.conv.1.0.weight False\n",
      "31 features.4.conv.1.1.weight False\n",
      "32 features.4.conv.1.1.bias False\n",
      "33 features.4.conv.2.weight False\n",
      "34 features.4.conv.3.weight False\n",
      "35 features.4.conv.3.bias False\n",
      "36 features.5.conv.0.0.weight False\n",
      "37 features.5.conv.0.1.weight False\n",
      "38 features.5.conv.0.1.bias False\n",
      "39 features.5.conv.1.0.weight False\n",
      "40 features.5.conv.1.1.weight False\n",
      "41 features.5.conv.1.1.bias False\n",
      "42 features.5.conv.2.weight False\n",
      "43 features.5.conv.3.weight False\n",
      "44 features.5.conv.3.bias False\n",
      "45 features.6.conv.0.0.weight False\n",
      "46 features.6.conv.0.1.weight False\n",
      "47 features.6.conv.0.1.bias False\n",
      "48 features.6.conv.1.0.weight False\n",
      "49 features.6.conv.1.1.weight False\n",
      "50 features.6.conv.1.1.bias False\n",
      "51 features.6.conv.2.weight False\n",
      "52 features.6.conv.3.weight False\n",
      "53 features.6.conv.3.bias False\n",
      "54 features.7.conv.0.0.weight False\n",
      "55 features.7.conv.0.1.weight False\n",
      "56 features.7.conv.0.1.bias False\n",
      "57 features.7.conv.1.0.weight False\n",
      "58 features.7.conv.1.1.weight False\n",
      "59 features.7.conv.1.1.bias False\n",
      "60 features.7.conv.2.weight False\n",
      "61 features.7.conv.3.weight False\n",
      "62 features.7.conv.3.bias False\n",
      "63 features.8.conv.0.0.weight False\n",
      "64 features.8.conv.0.1.weight False\n",
      "65 features.8.conv.0.1.bias False\n",
      "66 features.8.conv.1.0.weight False\n",
      "67 features.8.conv.1.1.weight False\n",
      "68 features.8.conv.1.1.bias False\n",
      "69 features.8.conv.2.weight False\n",
      "70 features.8.conv.3.weight False\n",
      "71 features.8.conv.3.bias False\n",
      "72 features.9.conv.0.0.weight False\n",
      "73 features.9.conv.0.1.weight False\n",
      "74 features.9.conv.0.1.bias False\n",
      "75 features.9.conv.1.0.weight False\n",
      "76 features.9.conv.1.1.weight False\n",
      "77 features.9.conv.1.1.bias False\n",
      "78 features.9.conv.2.weight False\n",
      "79 features.9.conv.3.weight False\n",
      "80 features.9.conv.3.bias False\n",
      "81 features.10.conv.0.0.weight False\n",
      "82 features.10.conv.0.1.weight False\n",
      "83 features.10.conv.0.1.bias False\n",
      "84 features.10.conv.1.0.weight False\n",
      "85 features.10.conv.1.1.weight False\n",
      "86 features.10.conv.1.1.bias False\n",
      "87 features.10.conv.2.weight False\n",
      "88 features.10.conv.3.weight False\n",
      "89 features.10.conv.3.bias False\n",
      "90 features.11.conv.0.0.weight False\n",
      "91 features.11.conv.0.1.weight False\n",
      "92 features.11.conv.0.1.bias False\n",
      "93 features.11.conv.1.0.weight False\n",
      "94 features.11.conv.1.1.weight False\n",
      "95 features.11.conv.1.1.bias False\n",
      "96 features.11.conv.2.weight False\n",
      "97 features.11.conv.3.weight False\n",
      "98 features.11.conv.3.bias False\n",
      "99 features.12.conv.0.0.weight False\n",
      "100 features.12.conv.0.1.weight False\n",
      "101 features.12.conv.0.1.bias False\n",
      "102 features.12.conv.1.0.weight False\n",
      "103 features.12.conv.1.1.weight False\n",
      "104 features.12.conv.1.1.bias False\n",
      "105 features.12.conv.2.weight False\n",
      "106 features.12.conv.3.weight False\n",
      "107 features.12.conv.3.bias False\n",
      "108 features.13.conv.0.0.weight False\n",
      "109 features.13.conv.0.1.weight False\n",
      "110 features.13.conv.0.1.bias False\n",
      "111 features.13.conv.1.0.weight False\n",
      "112 features.13.conv.1.1.weight False\n",
      "113 features.13.conv.1.1.bias False\n",
      "114 features.13.conv.2.weight False\n",
      "115 features.13.conv.3.weight False\n",
      "116 features.13.conv.3.bias False\n",
      "117 features.14.conv.0.0.weight False\n",
      "118 features.14.conv.0.1.weight False\n",
      "119 features.14.conv.0.1.bias False\n",
      "120 features.14.conv.1.0.weight False\n",
      "121 features.14.conv.1.1.weight False\n",
      "122 features.14.conv.1.1.bias False\n",
      "123 features.14.conv.2.weight False\n",
      "124 features.14.conv.3.weight False\n",
      "125 features.14.conv.3.bias False\n",
      "126 features.15.conv.0.0.weight False\n",
      "127 features.15.conv.0.1.weight False\n",
      "128 features.15.conv.0.1.bias False\n",
      "129 features.15.conv.1.0.weight False\n",
      "130 features.15.conv.1.1.weight False\n",
      "131 features.15.conv.1.1.bias False\n",
      "132 features.15.conv.2.weight False\n",
      "133 features.15.conv.3.weight False\n",
      "134 features.15.conv.3.bias False\n",
      "135 features.16.conv.0.0.weight False\n",
      "136 features.16.conv.0.1.weight False\n",
      "137 features.16.conv.0.1.bias False\n",
      "138 features.16.conv.1.0.weight False\n",
      "139 features.16.conv.1.1.weight False\n",
      "140 features.16.conv.1.1.bias False\n",
      "141 features.16.conv.2.weight False\n",
      "142 features.16.conv.3.weight False\n",
      "143 features.16.conv.3.bias False\n",
      "144 features.17.conv.0.0.weight False\n",
      "145 features.17.conv.0.1.weight False\n",
      "146 features.17.conv.0.1.bias False\n",
      "147 features.17.conv.1.0.weight False\n",
      "148 features.17.conv.1.1.weight False\n",
      "149 features.17.conv.1.1.bias False\n",
      "150 features.17.conv.2.weight False\n",
      "151 features.17.conv.3.weight False\n",
      "152 features.17.conv.3.bias False\n",
      "153 features.18.0.weight True\n",
      "154 features.18.1.weight True\n",
      "155 features.18.1.bias True\n",
      "156 classifier.1.weight True\n",
      "157 classifier.1.bias True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                    [-1, 2]           2,562\n",
      "================================================================\n",
      "Total params: 2,226,434\n",
      "Trainable params: 414,722\n",
      "Non-trainable params: 1,811,712\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.86\n",
      "Params size (MB): 8.49\n",
      "Estimated Total Size (MB): 161.93\n",
      "----------------------------------------------------------------\n",
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Transfer the model to GPU if available\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print the summary of the model\n",
    "print('Model Summary:-\\n')\n",
    "\n",
    "# Enumerate through the model parameters and print them\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad)  # Print the parameter number, name, and whether it requires gradient\n",
    "\n",
    "# Use the summary function from the torchsummary package to print a detailed summary of the model\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e44bc0e3-ca1e-4970-9f01-9a0f02af2b74",
   "metadata": {
    "id": "e44bc0e3-ca1e-4970-9f01-9a0f02af2b74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# CrossEntropyLoss is used for multi-class classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "# Stochastic Gradient Descent (SGD) is used as the optimizer\n",
    "# The learning rate is set to 0.001 and the momentum is set to 0.9\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "# The learning rate will be reduced by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4843a46-0621-4d7a-8a8e-5dcbf7a62253",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4843a46-0621-4d7a-8a8e-5dcbf7a62253",
    "outputId": "7a02e887-843d-43ce-be71-c63ee80225f4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.5720 Acc: 0.6856\n",
      "valid Loss: 0.4469 Acc: 0.7788\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4769 Acc: 0.7647\n",
      "valid Loss: 0.4367 Acc: 0.8077\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.4832 Acc: 0.7797\n",
      "valid Loss: 0.4046 Acc: 0.8173\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.4677 Acc: 0.7754\n",
      "valid Loss: 0.4911 Acc: 0.7596\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.4700 Acc: 0.7701\n",
      "valid Loss: 0.4255 Acc: 0.8269\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.4600 Acc: 0.7850\n",
      "valid Loss: 0.5309 Acc: 0.7500\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.4546 Acc: 0.7818\n",
      "valid Loss: 0.5605 Acc: 0.7212\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.3826 Acc: 0.8267\n",
      "valid Loss: 0.4143 Acc: 0.7981\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.4016 Acc: 0.8021\n",
      "valid Loss: 0.4389 Acc: 0.7981\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.4432 Acc: 0.7925\n",
      "valid Loss: 0.4117 Acc: 0.8269\n",
      "\n",
      "Training complete in 7m 2s\n",
      "Best val Acc: 0.826923\n",
      "\n",
      "Saving the model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining:-\\n\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    \"\"\"\n",
    "    This function trains the model for a specified number of epochs and optimizes it.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to train.\n",
    "    criterion (torch.nn.modules.loss): The loss function.\n",
    "    optimizer (torch.optim): The optimizer.\n",
    "    scheduler (torch.optim.lr_scheduler): The learning rate scheduler.\n",
    "    num_epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "    model (torch.nn.Module): The trained model.\n",
    "    \"\"\"\n",
    "    # Record the start time\n",
    "    since = time.time()\n",
    "\n",
    "    # Initialize the best model weights and accuracy\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Initialize the TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set the model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set the model to evaluation mode\n",
    "\n",
    "            # Initialize the running loss and correct predictions\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over the data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Transfer the inputs and labels to the device\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update the running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            # Step the scheduler if in training phase\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Calculate the epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = torch.tensor(running_corrects).double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record the training loss and accuracy for each phase in TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # Deep copy the model if the current accuracy is the best\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate and print the time taken for training\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bba67af5-8c16-45d0-91b2-cfcde4aa2b39",
   "metadata": {
    "id": "bba67af5-8c16-45d0-91b2-cfcde4aa2b39",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "# 'model_ft' is the trained model and 'PATH' is the location where the model will be saved\n",
    "torch.save(model_ft, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c4aa2f-90cb-45bd-9576-8e868d3509ca",
   "metadata": {
    "id": "93c4aa2f-90cb-45bd-9576-8e868d3509ca"
   },
   "outputs": [],
   "source": [
    "PATH=\"/content/drive/MyDrive/model_transfer.pth\"\n",
    "torch.save(model_ft, PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
