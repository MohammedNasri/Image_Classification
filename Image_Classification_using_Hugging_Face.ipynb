{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Zxzg6jX_9CDU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zxzg6jX_9CDU",
    "outputId": "46457005-9da5-493a-e827-68e51be1e987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages for working with datasets, transformers, TensorFlow, and evaluation tools.\n",
    "!pip -q install datasets\n",
    "!pip -q install transformers=='4.29.0'\n",
    "!pip -q install tensorflow=='2.15'  # At least this TensorFlow version is required to use the \"evaluate\" module.\n",
    "!pip -q install evaluate\n",
    "!pip -q install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nQCLE-zv9Z7Z",
   "metadata": {
    "id": "nQCLE-zv9Z7Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import evaluate\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import DefaultDataCollator\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gF4NHPhInv1Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF4NHPhInv1Y",
    "outputId": "9bd1917e-12ac-426a-9b92-49fc4143f3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2c0dfb-f470-4ff2-a476-91da5efc08b6",
   "metadata": {
    "id": "ca2c0dfb-f470-4ff2-a476-91da5efc08b6"
   },
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path, dest_path):\n",
    "    \"\"\"\n",
    "    Extracts a ZIP file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        zip_path (str): The file path to the ZIP file.\n",
    "        dest_path (str): The destination directory where the contents will be extracted.\n",
    "\n",
    "    This function opens the specified ZIP file in read mode and extracts all its contents\n",
    "    to the given destination directory.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)  # Extract all files to the destination directory\n",
    "\n",
    "# Usage\n",
    "extract_zip_file('/content/drive/MyDrive/data.zip', 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "doN0gK1_GNfJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2f70d35ddc504f8c9ce26c891de76c62",
      "b85d934a30f54467b0489f2a7f9d5259",
      "1cc21b0eff134dc9a0cb2daebf07695c",
      "7a9417db0c6e4c5a8624c03f63d952d4",
      "1f048ed19c304032af69e919bf466736",
      "e99dbf8fd47c45f6bdf31f69dc632c11",
      "3fdea3729cc44918af3bc2900cb6ed93",
      "39623c48df7043ceadeb5d2a8afeb000",
      "f49576f767924ca1a44f30ac60e212b7",
      "ed196235daf340ac97a2e57530dd0e40",
      "fe53ced3177f4a5693fe8cd03a8f2320",
      "d0c0fe7c2c114e9989b711522c93bbba",
      "ec46b3f1822545acaef03c49701c175b",
      "b5b87cb472704baf96a37edf0aa548cd",
      "f4ec00712d0e415c9d37e24164516e0a",
      "2881248e5d6047acbb40706806ca7661",
      "892bbe41c7434bdbb5f5fcd07857e75a",
      "781c1df277e44fc794bea3bd8266fb72",
      "7a0acdfff65a4cfc841936b0c9755329",
      "8c8095752ccb4352a6d38f0d0487a777",
      "f01156cd91104766a7b998ca27b098b8",
      "c55e60f18cb24936ab74b3d59d43f021",
      "589088e58b0e42b78f9f68d72d3b8131",
      "ce51cd5b373149799570ed925effd38e",
      "7ff84c7df8a542eba1bf708464cdb6ee",
      "3ff34d74a8ee4010a44683512ac1afac",
      "4c66f55e4f734c3284801c62f40189cb",
      "d5e725c9c5e74824b54b58d1dbd07f3a",
      "d5567322a1fd424ebed732897b69dfe3",
      "eb06e09183634ffbb5358297c1f55016",
      "288a5e4914674d3aa6bb0355ac3cab3e",
      "576a7d705ab645208da97cd4193e7436"
     ]
    },
    "id": "doN0gK1_GNfJ",
    "outputId": "b195b98e-1432-41ff-a7ae-8af01f4b8cac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f70d35ddc504f8c9ce26c891de76c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aROofqlS_OR",
   "metadata": {
    "id": "3aROofqlS_OR"
   },
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "y_train = pd.read_csv('/content/data/Y_train.csv')\n",
    "x_train = pd.read_csv('/content/data/X_train.csv')\n",
    "x_train = x_train.head(100)\n",
    "y_train = y_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82XNQWWuTDCZ",
   "metadata": {
    "id": "82XNQWWuTDCZ"
   },
   "outputs": [],
   "source": [
    "# Make sure that the dataframes are aligned\n",
    "assert len(y_train) == len(x_train), \"The number of labels does not match the number of image paths.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-9I9YOaTLbx",
   "metadata": {
    "id": "7-9I9YOaTLbx"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store the data\n",
    "image_file_paths = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "def load_images_and_labels(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Loads images and their corresponding labels, converts images to PyTorch tensors,\n",
    "    and stores the file paths, tensors, and labels in respective lists.\n",
    "\n",
    "    Args:\n",
    "        x_train (pd.DataFrame): DataFrame containing image file paths.\n",
    "        y_train (pd.DataFrame): DataFrame containing labels corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - image file paths, images as tensors, and labels.\n",
    "    \"\"\"\n",
    "    for i in range(len(y_train)):\n",
    "        # Construct the full image file path\n",
    "        image_file_path = '/content/' + x_train.loc[i, 'image_path']\n",
    "\n",
    "        # Load the image using PIL\n",
    "        image = Image.open(image_file_path)\n",
    "\n",
    "        # Get the corresponding label\n",
    "        label = y_train.loc[i, 'output']\n",
    "\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        image = ToTensor()(image)\n",
    "        image = torch.Tensor(image)  # Ensure the image is a PyTorch tensor\n",
    "\n",
    "        # Check that the image has been correctly transformed to a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            print(f\"Warning: Expected image to be a PyTorch tensor after transformation but got {type(image)}. Skipping this image.\")\n",
    "            continue\n",
    "\n",
    "        # Append the data to the lists\n",
    "        image_file_paths.append(image_file_path)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return image_file_paths, images, labels\n",
    "\n",
    "# Example usage\n",
    "image_file_paths, images, labels = load_images_and_labels(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UERpsrXeTNuh",
   "metadata": {
    "id": "UERpsrXeTNuh"
   },
   "outputs": [],
   "source": [
    "def create_dataset(image_file_paths, images, labels):\n",
    "    \"\"\"\n",
    "    Creates a dataset from given lists of image file paths, images, and labels.\n",
    "\n",
    "    Args:\n",
    "        image_file_paths (list): List of file paths to the images.\n",
    "        images (list): List of images as PyTorch tensors.\n",
    "        labels (list): List of labels corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face `Dataset` object containing the image file paths, images, and labels.\n",
    "    \"\"\"\n",
    "    # Create the dataset from the provided lists\n",
    "    dataset = Dataset.from_dict({\n",
    "        'image_file_path': image_file_paths,\n",
    "        'image': images,\n",
    "        'labels': labels\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "dataset_train = create_dataset(image_file_paths, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hBQVYdaTMLxo",
   "metadata": {
    "id": "hBQVYdaTMLxo"
   },
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9P9OoSGTa2B",
   "metadata": {
    "id": "y9P9OoSGTa2B"
   },
   "outputs": [],
   "source": [
    "def display_random_images(dataset, num_images=4, labels_names=None):\n",
    "    \"\"\"\n",
    "    Displays a grid of random images from the dataset along with their labels.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset containing images and labels.\n",
    "        num_images (int, optional): The number of images to display. Default is 4.\n",
    "        labels_names (dict, optional): A dictionary mapping label indices to label names. Default is None.\n",
    "\n",
    "    This function randomly selects a specified number of images from the dataset and\n",
    "    displays them in a grid format using Matplotlib. The images are converted to\n",
    "    PyTorch tensors if they are not already, and their dimensions are rearranged for display.\n",
    "    \"\"\"\n",
    "    num_rows = 2\n",
    "    num_cols = np.ceil(num_images / num_rows).astype(int)\n",
    "\n",
    "    # Set up the figure size based on the number of columns and rows\n",
    "    plt.figure(figsize=(num_cols * 3, num_rows * 3))\n",
    "\n",
    "    # Randomly select indices from the dataset\n",
    "    indices = np.random.choice(range(len(dataset)), size=num_images, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(indices, 1):\n",
    "        idx = int(idx)\n",
    "        image = dataset[idx]['image']\n",
    "        label = dataset[idx]['labels']\n",
    "        label_name = labels_names.get(label, \"Unknown\") if labels_names else label\n",
    "\n",
    "        # Convert the image to a PyTorch tensor if it's a list\n",
    "        if isinstance(image, list):\n",
    "            image = torch.Tensor(image)\n",
    "\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            # Rearrange the dimensions of the image for plotting\n",
    "            image = image.permute(1, 2, 0)\n",
    "\n",
    "            plt.subplot(num_rows, num_cols, i)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"{label_name} ({label})\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "display_random_images(dataset_train, num_images=4, labels_names={0: 'NotQuarry', 1: 'Quarry'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bR37v-YdTc65",
   "metadata": {
    "id": "bR37v-YdTc65"
   },
   "outputs": [],
   "source": [
    "id2label = {'0': 'NotQuarry', '1': 'Quarry'}\n",
    "label2id = {'NotQuarry':'0' , 'Quarry':'1'}\n",
    "labels = ['NotQuarry','Quarry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aEHb5toqTeff",
   "metadata": {
    "id": "aEHb5toqTeff"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to split.\n",
    "        test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A dictionary containing the training and testing datasets.\n",
    "    \"\"\"\n",
    "    # Split the dataset into training and testing sets\n",
    "    split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "    return split_dataset\n",
    "\n",
    "# Example usage\n",
    "dataset_train = split_dataset(dataset_train, test_size=0.2)\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J1FPYgXQTjC3",
   "metadata": {
    "id": "J1FPYgXQTjC3"
   },
   "outputs": [],
   "source": [
    "def initialize_image_processor(checkpoint):\n",
    "    \"\"\"\n",
    "    Initializes an image processor using a pre-trained model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint (str): The model checkpoint to use for the image processor.\n",
    "\n",
    "    Returns:\n",
    "        AutoImageProcessor: An initialized image processor.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained image processor from the specified checkpoint\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    return image_processor\n",
    "\n",
    "# Example usage\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = initialize_image_processor(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q72IOWbfTq5f",
   "metadata": {
    "id": "q72IOWbfTq5f"
   },
   "outputs": [],
   "source": [
    "def create_image_transforms(image_processor):\n",
    "    \"\"\"\n",
    "    Creates a composition of image transforms based on the specifications of an image processor.\n",
    "\n",
    "    Args:\n",
    "        image_processor (AutoImageProcessor): The image processor containing the mean, std, and size specifications.\n",
    "\n",
    "    Returns:\n",
    "        Compose: A composition of image transforms.\n",
    "    \"\"\"\n",
    "    # Define the normalization transform using the image processor's mean and std values\n",
    "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "    # Determine the size for the RandomResizedCrop transform\n",
    "    size = (\n",
    "        image_processor.size[\"shortest_edge\"]\n",
    "        if \"shortest_edge\" in image_processor.size\n",
    "        else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    )\n",
    "\n",
    "    # Create a composition of transforms: RandomResizedCrop, ToTensor, and Normalize\n",
    "    transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "    return transforms\n",
    "\n",
    "# Example usage\n",
    "_transforms = create_image_transforms(image_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EfC6Zs3gTuVN",
   "metadata": {
    "id": "EfC6Zs3gTuVN"
   },
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    \"\"\"\n",
    "    Applies a series of image transformations to the images in the provided examples.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the examples, where each example has an \"image\" key.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with transformed images under the \"pixel_values\" key and the original \"image\" key removed.\n",
    "    \"\"\"\n",
    "    transformed_images = []\n",
    "\n",
    "    for img in examples[\"image\"]:\n",
    "        # Check if img is a PIL Image object\n",
    "        if isinstance(img, Image.Image):\n",
    "            # Convert image to RGB and apply transformations\n",
    "            transformed_images.append(_transforms(img.convert(\"RGB\")))\n",
    "        else:\n",
    "            # Print a warning if the image is not a PIL Image\n",
    "            print(f\"Warning: Expected img to be a PIL Image but got {type(img)}. Skipping this image.\")\n",
    "\n",
    "    # Add transformed images to examples with the key \"pixel_values\"\n",
    "    examples[\"pixel_values\"] = transformed_images\n",
    "    # Remove the original \"image\" key from examples\n",
    "    del examples[\"image\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "# Apply the transforms function to the dataset\n",
    "dataset_train = dataset_train.with_transform(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K1PxcfPAT0-N",
   "metadata": {
    "id": "K1PxcfPAT0-N"
   },
   "outputs": [],
   "source": [
    "def initialize_data_collator():\n",
    "    \"\"\"\n",
    "    Initializes a default data collator for use in data loading and batching.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDataCollator: An instance of DefaultDataCollator.\n",
    "    \"\"\"\n",
    "    # Create an instance of DefaultDataCollator\n",
    "    data_collator = DefaultDataCollator()\n",
    "    return data_collator\n",
    "\n",
    "def load_accuracy_metric():\n",
    "    \"\"\"\n",
    "    Loads the accuracy metric for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Metric: An accuracy metric object.\n",
    "    \"\"\"\n",
    "    # Load the accuracy metric using the evaluate library\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "data_collator = initialize_data_collator()\n",
    "accuracy = load_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PPJUSH1vT2oV",
   "metadata": {
    "id": "PPJUSH1vT2oV"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for model predictions.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): A tuple containing the model predictions and the true labels.\n",
    "                           - predictions (numpy.ndarray): The model's output predictions.\n",
    "                           - labels (numpy.ndarray): The true labels for the evaluation dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed accuracy metric.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Get the index of the highest predicted value for each prediction (the predicted class)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the accuracy metric using the predictions and the true labels\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ZmbHrBwT3zd",
   "metadata": {
    "id": "3ZmbHrBwT3zd"
   },
   "outputs": [],
   "source": [
    "def initialize_model(checkpoint, num_labels, id2label, label2id):\n",
    "    \"\"\"\n",
    "    Initializes a pre-trained image classification model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint (str): The model checkpoint to use for initialization.\n",
    "        num_labels (int): The number of labels (classes) for the classification task.\n",
    "        id2label (dict): A dictionary mapping label IDs to label names.\n",
    "        label2id (dict): A dictionary mapping label names to label IDs.\n",
    "\n",
    "    Returns:\n",
    "        AutoModelForImageClassification: An initialized image classification model.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained image classification model with specified parameters\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "num_labels = len(labels)\n",
    "\n",
    "model = initialize_model(checkpoint, num_labels, id2label, label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_KlFX8gT6wd",
   "metadata": {
    "id": "X_KlFX8gT6wd"
   },
   "outputs": [],
   "source": [
    "def create_training_arguments(output_dir, learning_rate=5e-5, train_batch_size=16, eval_batch_size=16, num_epochs=3, warmup_ratio=0.1, logging_steps=10):\n",
    "    \"\"\"\n",
    "    Creates training arguments for model training.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): The directory where the model checkpoints and logs will be saved.\n",
    "        learning_rate (float, optional): The learning rate for the optimizer. Default is 5e-5.\n",
    "        train_batch_size (int, optional): The batch size per device for training. Default is 16.\n",
    "        eval_batch_size (int, optional): The batch size per device for evaluation. Default is 16.\n",
    "        num_epochs (int, optional): The number of training epochs. Default is 3.\n",
    "        warmup_ratio (float, optional): The warmup ratio for the learning rate schedule. Default is 0.1.\n",
    "        logging_steps (int, optional): The number of steps between logging. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        TrainingArguments: An instance of TrainingArguments with the specified parameters.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=logging_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Example usage\n",
    "training_args = create_training_arguments(\n",
    "    output_dir=\"./ORIS_classification\",\n",
    "    learning_rate=5e-5,\n",
    "    train_batch_size=16,\n",
    "    eval_batch_size=16,\n",
    "    num_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6U2jJ97T8fv",
   "metadata": {
    "id": "j6U2jJ97T8fv"
   },
   "outputs": [],
   "source": [
    "def create_trainer(model, training_args, data_collator, train_dataset, eval_dataset, tokenizer, compute_metrics):\n",
    "    \"\"\"\n",
    "    Creates a Trainer instance for training and evaluating the model.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): The pre-trained model to be trained and evaluated.\n",
    "        training_args (TrainingArguments): The training arguments specifying the training configuration.\n",
    "        data_collator (DataCollator): The data collator used for batching data.\n",
    "        train_dataset (Dataset): The training dataset.\n",
    "        eval_dataset (Dataset): The evaluation dataset.\n",
    "        tokenizer (AutoImageProcessor): The tokenizer or image processor for preprocessing the data.\n",
    "        compute_metrics (function): The function used to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: An instance of Trainer configured with the provided parameters.\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "# Example usage\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_train[\"train\"],\n",
    "    eval_dataset=dataset_train[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xx4ZIz6ZS79J",
   "metadata": {
    "id": "xx4ZIz6ZS79J"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NIozp73xdjq3",
   "metadata": {
    "id": "NIozp73xdjq3"
   },
   "outputs": [],
   "source": [
    "#Pushing the model\n",
    "trainer.model.push_to_hub(\"MohammedNasri/ORIS_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import evaluate\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import DefaultDataCollator\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "drive.mount('/content/drive')\n",
    "def extract_zip_file(zip_path, dest_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "extract_zip_file('/content/drive/MyDrive/data.zip', 'data')\n",
    "notebook_login()\n",
    "y_train = pd.read_csv('/content/data/Y_train.csv')\n",
    "x_train = pd.read_csv('/content/data/X_train.csv')\n",
    "x_train = x_train.head(100)\n",
    "y_train = y_train.head(100)\n",
    "# Initialize lists to store the data\n",
    "image_file_paths = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "def load_images_and_labels(x_train, y_train):\n",
    "    for i in range(len(y_train)):\n",
    "        image_file_path = '/content/' + x_train.loc[i, 'image_path']\n",
    "        image = Image.open(image_file_path)\n",
    "        label = y_train.loc[i, 'output']\n",
    "        image = ToTensor()(image)\n",
    "        image = torch.Tensor(image)  \n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            print(f\"Warning: Expected image to be a PyTorch tensor after transformation but got {type(image)}. Skipping this image.\")\n",
    "            continue\n",
    "        image_file_paths.append(image_file_path)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    return image_file_paths, images, labels\n",
    "image_file_paths, images, labels = load_images_and_labels(x_train, y_train)\n",
    "def create_dataset(image_file_paths, images, labels):\n",
    "    dataset = Dataset.from_dict({\n",
    "        'image_file_path': image_file_paths,\n",
    "        'image': images,\n",
    "        'labels': labels\n",
    "    })\n",
    "    return dataset\n",
    "dataset_train = create_dataset(image_file_paths, images, labels)\n",
    "def display_random_images(dataset, num_images=4, labels_names=None):\n",
    "    num_rows = 2\n",
    "    num_cols = np.ceil(num_images / num_rows).astype(int)\n",
    "    plt.figure(figsize=(num_cols * 3, num_rows * 3))\n",
    "    indices = np.random.choice(range(len(dataset)), size=num_images, replace=False)\n",
    "    for i, idx in enumerate(indices, 1):\n",
    "        idx = int(idx)\n",
    "        image = dataset[idx]['image']\n",
    "        label = dataset[idx]['labels']\n",
    "        label_name = labels_names.get(label, \"Unknown\") if labels_names else label\n",
    "        if isinstance(image, list):\n",
    "            image = torch.Tensor(image)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0)\n",
    "            plt.subplot(num_rows, num_cols, i)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"{label_name} ({label})\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_random_images(dataset_train, num_images=4, labels_names={0: 'NotQuarry', 1: 'Quarry'})\n",
    "id2label = {'0': 'NotQuarry', '1': 'Quarry'}\n",
    "label2id = {'NotQuarry':'0' , 'Quarry':'1'}\n",
    "labels = ['NotQuarry','Quarry']\n",
    "def split_dataset(dataset, test_size=0.2):\n",
    "    split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "    return split_dataset\n",
    "dataset_train = split_dataset(dataset_train, test_size=0.2)\n",
    "def initialize_image_processor(checkpoint):\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    return image_processor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = initialize_image_processor(checkpoint)\n",
    "def create_image_transforms(image_processor):\n",
    "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "    size = (\n",
    "        image_processor.size[\"shortest_edge\"]\n",
    "        if \"shortest_edge\" in image_processor.size\n",
    "        else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    )\n",
    "    transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "    return transforms\n",
    "\n",
    "# Example usage\n",
    "_transforms = create_image_transforms(image_processor)\n",
    "def transforms(examples):\n",
    "    transformed_images = []\n",
    "    for img in examples[\"image\"]:\n",
    "        if isinstance(img, Image.Image):\n",
    "            transformed_images.append(_transforms(img.convert(\"RGB\")))\n",
    "        else:\n",
    "            print(f\"Warning: Expected img to be a PIL Image but got {type(img)}. Skipping this image.\")\n",
    "\n",
    "    examples[\"pixel_values\"] = transformed_images\n",
    "    del examples[\"image\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "dataset_train = dataset_train.with_transform(transforms)\n",
    "\n",
    "def initialize_data_collator():\n",
    "\n",
    "    data_collator = DefaultDataCollator()\n",
    "    return data_collator\n",
    "\n",
    "def load_accuracy_metric():\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy\n",
    "data_collator = initialize_data_collator()\n",
    "accuracy = load_accuracy_metric()\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "def initialize_model(checkpoint, num_labels, id2label, label2id):\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "num_labels = len(labels)\n",
    "\n",
    "model = initialize_model(checkpoint, num_labels, id2label, label2id)\n",
    "def create_training_arguments(output_dir, learning_rate=5e-5, train_batch_size=16, eval_batch_size=16, num_epochs=3, warmup_ratio=0.1, logging_steps=10):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=logging_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Example usage\n",
    "training_args = create_training_arguments(\n",
    "    output_dir=\"./ORIS_classification\",\n",
    "    learning_rate=5e-5,\n",
    "    train_batch_size=16,\n",
    "    eval_batch_size=16,\n",
    "    num_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10\n",
    ")\n",
    "def create_trainer(model, training_args, data_collator, train_dataset, eval_dataset, tokenizer, compute_metrics):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_train[\"train\"],\n",
    "    eval_dataset=dataset_train[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.push_to_hub(\"MohammedNasri/ORIS_classification\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1cc21b0eff134dc9a0cb2daebf07695c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_ed196235daf340ac97a2e57530dd0e40",
      "placeholder": "​",
      "style": "IPY_MODEL_fe53ced3177f4a5693fe8cd03a8f2320",
      "value": ""
     }
    },
    "1f048ed19c304032af69e919bf466736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b5b87cb472704baf96a37edf0aa548cd",
      "style": "IPY_MODEL_f4ec00712d0e415c9d37e24164516e0a",
      "tooltip": ""
     }
    },
    "2881248e5d6047acbb40706806ca7661": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "288a5e4914674d3aa6bb0355ac3cab3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f70d35ddc504f8c9ce26c891de76c62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f01156cd91104766a7b998ca27b098b8",
       "IPY_MODEL_c55e60f18cb24936ab74b3d59d43f021",
       "IPY_MODEL_589088e58b0e42b78f9f68d72d3b8131",
       "IPY_MODEL_ce51cd5b373149799570ed925effd38e"
      ],
      "layout": "IPY_MODEL_3fdea3729cc44918af3bc2900cb6ed93"
     }
    },
    "39623c48df7043ceadeb5d2a8afeb000": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fdea3729cc44918af3bc2900cb6ed93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "3ff34d74a8ee4010a44683512ac1afac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c66f55e4f734c3284801c62f40189cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "576a7d705ab645208da97cd4193e7436": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "589088e58b0e42b78f9f68d72d3b8131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5567322a1fd424ebed732897b69dfe3",
      "placeholder": "​",
      "style": "IPY_MODEL_eb06e09183634ffbb5358297c1f55016",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "781c1df277e44fc794bea3bd8266fb72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a0acdfff65a4cfc841936b0c9755329",
      "placeholder": "​",
      "style": "IPY_MODEL_8c8095752ccb4352a6d38f0d0487a777",
      "value": "Connecting..."
     }
    },
    "7a0acdfff65a4cfc841936b0c9755329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a9417db0c6e4c5a8624c03f63d952d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_d0c0fe7c2c114e9989b711522c93bbba",
      "style": "IPY_MODEL_ec46b3f1822545acaef03c49701c175b",
      "value": true
     }
    },
    "7ff84c7df8a542eba1bf708464cdb6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "892bbe41c7434bdbb5f5fcd07857e75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c8095752ccb4352a6d38f0d0487a777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5b87cb472704baf96a37edf0aa548cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b85d934a30f54467b0489f2a7f9d5259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39623c48df7043ceadeb5d2a8afeb000",
      "placeholder": "​",
      "style": "IPY_MODEL_f49576f767924ca1a44f30ac60e212b7",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "c55e60f18cb24936ab74b3d59d43f021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c66f55e4f734c3284801c62f40189cb",
      "placeholder": "​",
      "style": "IPY_MODEL_d5e725c9c5e74824b54b58d1dbd07f3a",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "ce51cd5b373149799570ed925effd38e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_288a5e4914674d3aa6bb0355ac3cab3e",
      "placeholder": "​",
      "style": "IPY_MODEL_576a7d705ab645208da97cd4193e7436",
      "value": "Login successful"
     }
    },
    "d0c0fe7c2c114e9989b711522c93bbba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5567322a1fd424ebed732897b69dfe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5e725c9c5e74824b54b58d1dbd07f3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e99dbf8fd47c45f6bdf31f69dc632c11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2881248e5d6047acbb40706806ca7661",
      "placeholder": "​",
      "style": "IPY_MODEL_892bbe41c7434bdbb5f5fcd07857e75a",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "eb06e09183634ffbb5358297c1f55016": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec46b3f1822545acaef03c49701c175b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed196235daf340ac97a2e57530dd0e40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01156cd91104766a7b998ca27b098b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ff84c7df8a542eba1bf708464cdb6ee",
      "placeholder": "​",
      "style": "IPY_MODEL_3ff34d74a8ee4010a44683512ac1afac",
      "value": "Token is valid (permission: write)."
     }
    },
    "f49576f767924ca1a44f30ac60e212b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4ec00712d0e415c9d37e24164516e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "fe53ced3177f4a5693fe8cd03a8f2320": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
